<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta name="generator" content="jemdoc" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title>Zhang, Binjie (张斌杰)</title>
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">

  <!-- Left menu -->
  <td id="layout-menu">
    <div class="menu-category">Menu</div>
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="https://scholar.google.com/citations?user=Gk-tbbgAAAAJ&hl=en">Google Scholar</a></div>
    <div class="menu-item"><a href="https://github.com/binjiezhang">GitHub</a></div>
    <div class="menu-item"><a href="cv/cv.pdf">CV</a></div>
  </td>

  <!-- Main content -->
  <td id="layout-content">
  <div id="toptitle">
    <h1>Zhang, Binjie (张斌杰)</h1>
  </div>

  <!-- Avatar -->
  <img src="images/bio.jpg" alt="Binjie Zhang" style="float:right; width:160px; border-radius:8px; margin:0 0 10px 20px;" />

  <p>
  Ph.D. Candidate, Computer Science<br />
  <a href="https://sites.google.com/view/showlab">Show Lab</a>, National University of Singapore (NUS)<br />
  E-mail: <a href="mailto:binjie97@u.nus.edu">binjie97@u.nus.edu</a>
  </p>

  <h2>Biography</h2>

  <p>
  I am a Ph.D. candidate in Computer Science at the National University of Singapore (NUS),
  advised by <a href="https://cde.nus.edu.sg/ece/staff/shou-zheng-mike/">Assistant Prof. Mike Zheng Shou</a>
  in <a href="https://sites.google.com/view/showlab">Show Lab</a>.
  My research interests lie in <b>video understanding</b>, <b>vision-language models</b>, and <b>world models</b>,
  with a focus on <b>deployable and compatible visual representations</b>.
  </p>

  <p>
  Before joining NUS, I received my M.Eng. in Computer Science and Technology from
  <b>Tsinghua University</b>, supervised by
  <a href="https://scholar.google.com/citations?user=fYdxi2sAAAAJ&hl=en">Prof. Chun Yuan</a>,
  and my B.Eng. in Information Engineering from
  <b>East China University of Science and Technology (ECUST)</b>, where I ranked <b>1 / 92</b>.
  </p>

  <p>
  I spent several years as an AI research intern at <b>Tencent ARC Lab</b> (Shenzhen), working on:
  </p>
  <ul>
    <li><p><b>Compatible representation learning</b> and <b>model upgrades</b> for large-scale image/video retrieval systems.</p></li>
    <li><p><b>Cross-modality video understanding</b>, including video-text retrieval and temporal grounding.</p></li>
  </ul>

  <p>
  Broadly, my research aims to make visual and multimodal models easier to upgrade, reuse, and deploy
  in real systems, while enabling embodied and egocentric agents to anticipate both actions and visual futures.
  </p>

  <h2>News</h2>
  <ul>
    <li><p>[2026] Submitted <b>GR-MCT</b> (tool-using agents with multi-modal CoT) and <b>PRCFC</b> (lifelong imitation learning) to CVPR 2026 as first author.</p></li>
    <li><p>[2025] <b>Ego-centric Predictive Model Conditioned on Hand Trajectories</b> submitted to ICLR 2025.</p></li>
    <li><p>[2023] <b>Darwinian Model Upgrades</b> accepted to AAAI 2023.</p></li>
    <li><p>[2022] <b>Towards Universal Backward-Compatible Representation Learning</b> accepted to IJCAI 2022 (long oral).</p></li>
    <li><p>[2022] <b>Hot-Refresh Model Upgrades</b> accepted to ICLR 2022, and received the <b>Tencent Technology Breakthrough Award</b>
           and <b>SZCCF Science and Technology Award</b>.</p></li>
  </ul>

<h2>Selected Publications</h2>
<p>(* indicates first author or core contribution)</p>

<ol>

  <li>
    <p>
    <b>GR-MCT: Group-Relative Reasoning with Multi-Modal CoT for Tool-Using Agents</b>*<br />
    <i>Binjie Zhang</i>, Mike Zheng Shou<br />
    CVPR, under submission, 2026.
    </p>
  </li>

  <li>
    <p>
    <b>PRCFC: Lifelong Imitation Learning via Prototype Replay and Coarse-to-Fine Compatibility</b>*<br />
    <i>Binjie Zhang</i>, Mike Zheng Shou<br />
    CVPR, under submission, 2026.
    </p>
  </li>

  <li>
    <p>
    <b>Ego-centric Predictive Model Conditioned on Hand Trajectories</b>*<br />
    <i>Binjie Zhang</i>, Yixiao Ge, Yu Li, Chun Yuan, Ying Shan, Mike Zheng Shou<br />
    ICLR, under review, 2025.<br />
    [<a href="https://openreview.net/pdf?id=GTFISI3Clc">Paper</a>]
    </p>
  </li>

  <li>
    <p>
    <b>TaCA: Upgrading Your Visual Foundation Model with a Task-Agnostic Compatible Adapter</b>*<br />
    <i>Binjie Zhang</i>, Yixiao Ge, Yantao Shen, Mike Zheng Shou<br />
    arXiv, 2023.<br />
    [<a href="https://arxiv.org/pdf/2306.12642">Paper</a>]
    </p>
  </li>

  <li>
    <p>
    <b>Darwinian Model Upgrades: Model Evolving with Selective Compatibility</b>*<br />
    <i>Binjie Zhang</i>, Yixiao Ge, Yueyu Hu, Ting Chen, Chun Yuan<br />
    AAAI, 2023.<br />
    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25447">Paper</a>]
    </p>
  </li>

  <li>
    <p>
    <b>Towards Universal Backward-Compatible Representation Learning</b>*<br />
    <i>Binjie Zhang</i>, Yixiao Ge, Yu Li, Chun Yuan<br />
    IJCAI (long oral), 2022.<br />
    [<a href="https://arxiv.org/pdf/2203.01583">Paper</a>]
    </p>
  </li>

  <li>
    <p>
    <b>Hot-Refresh Model Upgrades with Regression-Alleviating Compatible Training in Image Retrieval</b>*<br />
    <i>Binjie Zhang</i>, Yixiao Ge, Yu Li, Yantao Shen, Chun Yuan, Ying Shan<br />
    ICLR, 2022.<br />
    [<a href="https://openreview.net/forum?id=HTp-6yLGGX">Paper</a>]
    </p>
  </li>

</ol>

<p>
A more complete and up-to-date list can be found on my
<a href="https://scholar.google.com/citations?user=Gk-tbbgAAAAJ&hl=en">Google Scholar</a>.
</p>

  <!-- Projects section -->
  <h2>Projects</h2>

  <table class="imgtable">
  <tr>
    <td><img src="images/prcfc.pdf" alt="PRCFC project" width="180px" /></td>
    <td>
      <p>
      <b>Robot Lifelong Learning (PRCFC)</b><br />
      Developed PRCFC, a lightweight framework for lifelong imitation learning that uses compact prototype replay and
      coarse-to-fine compatibility regularization to reduce forgetting and improve cross-task transfer on the LIBERO benchmark.
      </p>
    </td>
  </tr>
  </table>

  <table class="imgtable">
  <tr>
    <td><img src="images/egopm.pdf" alt="Ego-centric Predictive Model" width="180px" /></td>
    <td>
      <p>
      <b>Ego-Centric Predictive Model</b><br />
      Designed a two-stage model that predicts future hand trajectories and uses them to guide a Latent Diffusion Model
      for egocentric future video generation, achieving state-of-the-art performance on Ego4D, BridgeData, and RLBench.
      </p>
    </td>
  </tr>
  </table>

  <table class="imgtable">
  <tr>
    <td><img src="images/taca.pdf" alt="TaCA Adapter" width="180px" /></td>
    <td>
      <p>
      <b>Task-Agnostic Compatible Adapter (TaCA)</b><br />
      Proposed TaCA, a parameter-efficient adapter that enables seamless upgrades between visual foundation models
      (e.g., CLIP variants) without retraining downstream tasks, validated on large-scale video-text and video recognition benchmarks.
      </p>
    </td>
  </tr>
  </table>

  <table class="imgtable">
  <tr>
    <td><img src="images/iclr_ract.jpeg" alt="Hot-Refresh Model Upgrades" width="180px" /></td>
    <td>
      <p>
      <b>Hot-Refresh Model Upgrades</b><br />
      Studied model regression in hot-refresh upgrades for large-scale image retrieval systems, proposing training regularization
      and uncertainty-based backfilling strategies to maintain compatibility without re-encoding existing galleries.
      </p>
    </td>
  </tr>
  </table>

  <h2>Research Experience</h2>
  <ul>
    <li>
      <p>
      <b>Ph.D. Candidate, Show Lab @ NUS</b><br />
      Jan. 2023 &ndash; Present, Singapore<br />
      Advisor: Mike Zheng Shou<br />
      Research on video understanding, vision-language models, world models, and compatible visual representations.
      </p>
    </li>

    <li>
      <p>
      <b>AI Research Intern, Tencent ARC Lab &ndash; Compatible Representation Learning Group</b><br />
      2020 &ndash; 2023, Shenzhen, China<br />
      Advisors: Yixiao Ge, Yantao Shen<br />
      Worked on backward-compatible representation learning and model upgrades for large-scale image and video retrieval systems,
      including multi-GPU PyTorch pipelines and hot-refresh deployment strategies.
      </p>
    </li>

    <li>
      <p>
      <b>AI Research Intern, Tencent ARC Lab &ndash; Cross-Modality Video Understanding Group</b><br />
      2019 &ndash; 2020, Shenzhen, China<br />
      Advisors: Yu Li, Ying Shan<br />
      Developed multimodal models for video-text retrieval and temporal grounding on large-scale video-language datasets.
      </p>
    </li>
  </ul>

  <h2>Education</h2>
  <ul>
    <li>
      <p>
      <b>National University of Singapore (NUS)</b><br />
      Ph.D. in Computer Science, Jan. 2023 &ndash; Jan. 2027 (expected), Singapore<br />
      Advisor: Mike Zheng Shou
      </p>
    </li>

    <li>
      <p>
      <b>Tsinghua University (THU)</b><br />
      M.Eng. in Computer Science and Technology, Aug. 2019 &ndash; Jul. 2022, Beijing, China<br />
      Advisor: Chun Yuan<br />
      Research focus: compatible representation learning; cross-modal video understanding.
      </p>
    </li>

    <li>
      <p>
      <b>East China University of Science and Technology (ECUST)</b><br />
      B.Eng. in Information Engineering, Aug. 2015 &ndash; Jul. 2019, Shanghai, China<br />
      Cumulative GPA: 3.77 / 4.00, Ranking: 1 / 92.
      </p>
    </li>
  </ul>

  <h2>Selected Awards</h2>
  <ul>
    <li><p><b>Tencent Technology Breakthrough Award</b> &ndash; Hot-Refresh Model Upgrades, 2022.</p></li>
    <li><p><b>SZCCF Science and Technology Award</b> &ndash; Efficient Model Upgrades, 2022.</p></li>
    <li><p><b>Excellent Master Degree Graduate in Beijing</b> &amp; <b>Outstanding Master's Graduation Thesis</b>, 2022.</p></li>
    <li><p><b>Annual College Personage Award</b> (highest student honor in ECUST), 2018.</p></li>
    <li><p><b>National Scholarship for Undergraduates</b> (twice), Ministry of Education of China, 2016 &amp; 2017.</p></li>
  </ul>

  <h2>Academic Service</h2>
  <ul>
    <li><p>Reviewer: ECCV, ICCV, AAAI, IJCAI, CVPR, ICLR</p></li>
  </ul>

  <p>
  &copy; Binjie Zhang &middot; Last updated: Nov. 2025
  </p>

  </td>
</tr>
</table>
</body>
</html>
